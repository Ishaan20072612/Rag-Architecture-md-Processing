# -*- coding: utf-8 -*-
"""Tennis_Details.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14KnXAsYnOYPinmcBkmDOIt19_EgiLRvp
"""

pip install langchain_community

import os
from langchain.document_loaders import TextLoader

loader = TextLoader("/content/tennis_details.md")
text_doc = loader.load()
print(text_doc)

print(text_doc[0].page_content)

from langchain_text_splitters import MarkdownHeaderTextSplitter

split_condition = [ ( "##" , "Title" ) ]
splitter = MarkdownHeaderTextSplitter(headers_to_split_on=split_condition)
# print(splitter)
doc_spliti = splitter.split_text(text_doc[0].page_content)
print(doc_spliti)

text_chunks = []
for i in doc_spliti:
  text_chunks.append(i.page_content)
print(text_chunks)

from sentence_transformers import SentenceTransformer

embedding_model = SentenceTransformer("all-MiniLM-L6-v2")

def embed_chunk(chunk):
  return embedding_model.encode([chunk] , normalize_embeddings=True)
sample_embeddings = embed_chunk(text_chunks[6])
print(sample_embeddings)

from langchain.vectorstores import Chroma

pip install chromadb

from langchain.embeddings import HuggingFaceEmbeddings
vector_db = Chroma.from_texts(text_chunks , HuggingFaceEmbeddings(model_name = "all-MiniLM-L6-v2") , persist_directory="/tmp/chromadb/")

vector_db._collection.get(include = ['embeddings','documents'])

from transformers import pipeline
from huggingface_hub import login
login()

pipe = pipeline("text-generation" , model = "meta-llama/Llama-3.2-1B")

print(pipe)

def retrieve_and_generate(query,threshold=1):
  search_results = vector_db.similarity_search_with_score(query,k=1)
  # print(search_results)
  if not search_results or search_results[0][1] > threshold:
    return "I don't know the answer"
  retrieve_context = search_results[0][0].page_content
  similarity_score = search_results[0][1]
  print(f"Similarity Score: {similarity_score}")
  print(f"Retrieved Context: {retrieve_context}")
  prompt = f"Answer the question using the given context \n Context: {retrieve_context} \n Question: {query} \n Answer:"
  # print(prompt)
  response = pipe(prompt , max_new_tokens = 100)
  print(response)
  return response[0]["generated_text"]

ques = "What is tennis?"
response = retrieve_and_generate(ques)
print(response)

ques = "What is gated models?"
response = retrieve_and_generate(ques)
print(response)